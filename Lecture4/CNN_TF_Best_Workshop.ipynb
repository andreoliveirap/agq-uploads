{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d8c20c6-94d8-444e-a6df-8713f1a242d9",
   "metadata": {},
   "source": [
    "# Best in Class?\n",
    "\n",
    "This playbook is concerned with trying to build a MNist numerical classifier which can be used to identify images not seen in the training dataset as well as possible.\n",
    "\n",
    "This means that we will make use of a lot of different techniques in the course.\n",
    "\n",
    "1. We will first train an Auto-Encoder to extract key features from our dataset in a positionally independent format.\n",
    "2. The encoder from this will be used to give us extracted key features from the input data in a positionally independent format.\n",
    "3. We will then train a combined CNN/Transformer model.\n",
    "4. We will evaluate model performance after training on unseen test data.\n",
    "5. Now there is a stable model, fine-tune the encoder for this task of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead4f361-bcc0-41a2-b1f5-88bf40ababdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b11f36-8a6c-4b1c-8233-5b8cbb5b7341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f2010c-adb6-406a-b8a3-3113270d0279",
   "metadata": {},
   "source": [
    "## Load the mnist dataset\n",
    "\n",
    "This time we will use a modified version of both the mnist training and test dataset.\n",
    "\n",
    "This means that we should get a model which is able to extract positionally invariant input features and then work out the class of the image based on these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c00ba0-8a8e-44f2-93c5-b3237a4ffcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"mps\")#\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load MNIST dataset without normalization\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "# Load entire dataset in one batch\n",
    "data_loader = DataLoader(mnist_dataset, batch_size=len(mnist_dataset), shuffle=False)\n",
    "# Get all images in a single batch\n",
    "images, _ = next(iter(data_loader))  # Shape: (60000, 1, 28, 28)\n",
    "# Calculate mean and standard deviation\n",
    "mean = images.mean().item()\n",
    "std = images.std().item()\n",
    "\n",
    "# Transform: Convert images to tensors and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=10, expand=False),       # Random rotations up to 10deg\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random shifts (up to 10% of image size)\n",
    "    transforms.RandomResizedCrop(size=28, scale=(0.9, 1.1)),   # Random scaling (90% to 110% of original size)\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),      # Adjust brightness & contrast slightly\n",
    "    transforms.ToTensor(),  # Converts to [0, 1]\n",
    "    transforms.Normalize((mean,), (std,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Transform: Convert images to tensors and normalize\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees= ## FINISH_ME ## , expand=False),         # Random rotations up to 20deg\n",
    "    transforms.RandomAffine(degrees=0, translate= ## FINISH_ME ## ),  # Random shifts (up to 15% of image size)\n",
    "    transforms.RandomResizedCrop(size=28, scale= ## FINISH_ME ## ),   # Random scaling (95% to 115% of original size)\n",
    "    transforms.ColorJitter(brightness= ## FINISH_ME ##, contrast= ),      # Adjust brightness & contrast slightly by a factor of 0.2\n",
    "    transforms.ToTensor(),  # Converts to [0, 1]\n",
    "    transforms.Normalize((mean,), (std,))\n",
    "])\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df31df6b-849d-4603-8a47-50ae546a8cc8",
   "metadata": {},
   "source": [
    "## First Build and train a simple AutoEncoder\n",
    "\n",
    "This AutoEncoder will encode images into a latent-space and then decode them back into 'input-like' images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579eb59-239b-4243-9f10-f754b4439458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder: Conv Layers + FC Layers merged into a single Sequential\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # Output: [batch, 32, 14, 14]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # Output: [batch, 64, 7, 7]\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),  # Flatten before FC layers\n",
    "            nn.Linear( ## FINISH_ME ##\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)  # Latent space\n",
    "        )\n",
    "\n",
    "        # Decoder: FC Layers + Transposed Conv Layers merged into a single Sequential\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear( ## FINISH_ME ##\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (64, 7, 7)),  # Reshape before transposed conv\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # Output in range [0,1] for images\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d0b212-165f-45bf-a2db-6daba63401bc",
   "metadata": {},
   "source": [
    "### We will train our auto-encoder for 5 epochs.\n",
    "\n",
    "This model has to be 'good enough', but doesn't need to be extesively trained. The better it is the better it will behave later, but for now 5-epochs is good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42bd6b-ad00-4a2e-8ffc-42d87d694034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-train the Autoencoder\n",
    "autoencoder = Autoencoder().to(device)\n",
    "criterion_ae = nn.MSELoss()\n",
    "optimizer_ae = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(5):  # Pre-train for fewer epochs\n",
    "    autoencoder.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Per-batch\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Forward, Loss, Backwards, Step\n",
    "        optimizer_ae.zero_grad()\n",
    "        outputs = autoencoder(images)\n",
    "        loss = ## FINISH_ME ##\n",
    "        loss.backward()\n",
    "        optimizer_ae.step()\n",
    "\n",
    "        # Track loss, is our model improving?\n",
    "        total_loss += loss.item()\n",
    "    # Print stats per-epoch to show we're doing something\n",
    "    print(f'Autoencoder Epoch [{epoch+1}/5], Loss: {total_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee70cd-63d8-4b23-b2ff-c9347d81efe0",
   "metadata": {},
   "source": [
    "## Build Our Positionally-Invarient Transformer based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22352c0-04c0-41e1-b86a-9aa6005fd821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the encoder when training the Transformer\n",
    "# This isn't structly needed, but makes the model more stable when training\n",
    "for param in autoencoder.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c3e334-9eaf-41f6-99bd-8b9fb6d773a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer Classifier\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, encoder, num_heads=4, num_classes=10):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        \n",
    "        # We know the output from the encoder is a 32-dim LS\n",
    "        # Capture these for now\n",
    "        self.encoder = encoder\n",
    "        self.embedding_dim = 32\n",
    "        self.frozen_encoder = True\n",
    "\n",
    "        # Transformer Encoder Layer\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            # The model takes the encoded input and effectively 'learns' relationships between them\n",
    "            d_model=self.embedding_dim, nhead=num_heads, dim_feedforward=128, dropout=0.1\n",
    "        )\n",
    "        # This class wraps our individual transformer into a full 'model' with multiple layers\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer, num_layers=5)\n",
    "\n",
    "        # Classification Head\n",
    "        # These pieces encode down from the Transformer back to an image class\n",
    "        self.fc1 = nn.Linear(self.embedding_dim, 64)\n",
    "        self.fc2 = nn.Linear( ## FINISH_ME ## )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # We want to use a pre-trained positionally-invarient feature encoder\n",
    "        if self.frozen_encoder:\n",
    "            with torch.no_grad():\n",
    "                x = self.encoder(x)  # Use pre-trained encoder\n",
    "        else:\n",
    "            x = self.encoder(x)\n",
    "\n",
    "        # Adding a sequence dimension (sequence length = 1)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.squeeze(1)  # Remove sequence dimension\n",
    "\n",
    "        # 'Project down' to a decsision\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = ## FINISH_ME ##\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc3562-8310-4a13-9b95-1062749f42d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Transformer-based classifier\n",
    "model = TransformerClassifier(autoencoder.encoder).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Per-batch\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forwards, Loss, Backwards, Step\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = ## FINISH_ME ##\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track Loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Print some extra data as this can be slow to train\n",
    "        if i % 100 == 0:\n",
    "            print(f'Average loss for this Batch: {loss.item()/len(images):.4f}')\n",
    "\n",
    "    print(f'Transformer Classifier Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ddf78a-0a91-4fa8-8f68-ccb7118d13ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += ## FINISH_ME ##\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the MNIST test set: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b1229b-3458-4d13-9473-c42dc970bc3a",
   "metadata": {},
   "source": [
    "## Push the model to it's limits\n",
    "\n",
    "Now we have a 'stable' model, lets let the model further guide 'fine-tuning' the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f2161-4127-410c-9064-f97688a92e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-Freeze the encoder when training the Transformer\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "model.frozen_encoder = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452188f-1293-4a45-945c-c9d82550949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Transformer-based classifier\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Per-batch\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forwards, Loss, Backwards, Step\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss =  ## FINISH_ME ##\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track Loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Print some extra data as this can be slow to train\n",
    "        if i % 100 == 0:\n",
    "            print(f'Average loss for this Batch: {loss.item()/len(images):.4f}')\n",
    "\n",
    "    print(f'Transformer Classifier Epoch [{epoch+1}/5], Loss: {total_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7898f-5d33-43a0-8056-35d72c510777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += ## FINISH_ME ##\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the MNIST test set: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b8aea-3aa7-4078-9f08-34bacaebf5b1",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "You should find that this model is capable of ~90% acuuracy when evaluated over the noisy version of the mnist test dataset.\n",
    "This shows this model is better at classifying images not present in the original dataset, therefore it's more generic.\n",
    "\n",
    "You should find that once the model 'un-freezes' the encoder component of the classifier and trains that in combination with the transformer layer it should improve in terms of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028f289-60b2-4d80-a485-1d9baacc90c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
