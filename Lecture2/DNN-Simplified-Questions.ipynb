{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdf2910b-edb5-4641-8fa9-ce5384633c34",
   "metadata": {},
   "source": [
    "# DeMystifying Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a30eb8c-b51c-47d6-b466-0935f2bb807f",
   "metadata": {},
   "source": [
    "## This Notebook\n",
    "\n",
    "This notebook consists of 4 parts:\n",
    "\n",
    "\n",
    "### Part 1 - NumPY DNN\n",
    "\n",
    "This part will walk through with an example constructing a DNN(MLP) using numoy from scratch.\n",
    "\n",
    "This will demonstrate how the forward and back propagation work for a simple training of a controlled dataset.\n",
    "\n",
    "### Part 2 - PyTorch DNN\n",
    "\n",
    "This part will demonstrate repeating the process of building an MLP using the PyTorch API to repeat the same work.\n",
    "\n",
    "Hopefully this demonstrates the advantage of working with frameworks which do the heavy lifting of generating the back-propagation for us from scratch\n",
    "\n",
    "If in doubt the PyTorch docs are available here: https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "## Part 3 - PyTorch Classifier\n",
    "\n",
    "This part will demonstrate constructing a Classifier model to classify data from the mnist numerical dataset.\n",
    "\n",
    "We will also use an independent data sub-set to estimate the model accuracy after training.\n",
    "\n",
    "## Part 4 - Projecting beyond the Training Window (Bonus)\n",
    "\n",
    "This part of the notebook is a bonus part if you've been able to finish all of the work above.\n",
    "\n",
    "This is set out to answer the question. What happens when we project beyond our training window with our Sinusoid model\n",
    "\n",
    "## Marking\n",
    "\n",
    "You will get marks for completeing the different tasks within this notebook:\n",
    "\n",
    "Any code expected for you to complete will contain `## FINISH_ME ##` indicating the code isn't expected to run until you have completed it.\n",
    "\n",
    "I would recommend tackling the playbook in order from Part1 -> Part2 -> Part3 -> Part4.\n",
    "\n",
    "\n",
    "| <p align='left'> Title                         | <p align='left'> Parts | <p align='left'> Number of marks |\n",
    "| ------------------------------------- | ----- | --- |\n",
    "| <p align='left'> 1. Completing the NumPY DNN model      | <p align='left'>  2  | <p align='left'> 2 |\n",
    "| <p align='left'> 2. Training the NumPY DNN model & verifying by prediction | <p align='left'>  2  | <p align='left'> 1 |\n",
    "| <p align='left'> 3. Construct a PyTorch DNN model       | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 4. Train the PyTorch DNN model & verify using evaluate    | <p align='left'>  2  | <p align='left'> 1 |\n",
    "| <p align='left'> 5. Examine the MNist dataset           | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 6. Evaluate pre-trained model accuracy | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 7. Build PyTorch Classifier            | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 8. Train the PyTorch Classifier        | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 9. Estimate the PyTorch model Classifier Accuracy | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> **Bonus 1:** Projecting both DNN models beyond the training window | <p align='left'>  2 | <p align='left'> 1 |\n",
    "| <p align='left'> **Total** | | <p align='left'> max **10** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e3d457-006a-48bb-96e8-c62fdab85d06",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Part 1 - NumPy DNN\n",
    "This part of the notebook walks you through building a DNN from scratch using nothing but numpy\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b4e8c6-e86d-46da-819f-e4837de0939d",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 1 - Imports and Globals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de5974f-5bf2-4144-861b-c41a3241ceab",
   "metadata": {},
   "source": [
    "First we're going to import the numpy modules and pyplot modules to allow us to manipulate and plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0fbe31-cbe9-4547-aa0a-7225285d3afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaaf3ed-b0c8-44fb-8aa2-f4c8f4dfcaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility in Science is critial, in computing it's often just a convenience\n",
    "_FIXED_SEED=12345\n",
    "np.random.seed(_FIXED_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0b195-12be-4a6e-8038-1856f11d3b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We haven't made any attempt to optimize our simple model so let's give it plenty of 'time' to train\n",
    "epochs = 100000\n",
    "# We know that the model can potentially be unstable, so let's take small steps toward the 'minima'\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c7c41-ce94-468d-a955-2ba5d5dc304f",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 1 - Build our model using numPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1fd953-4d2b-42d5-92e2-62fbf8eae245",
   "metadata": {},
   "source": [
    "You will need to complete this module which builds a short DNN, or MLP which consists of fully interconnected nodes.\n",
    "\n",
    "Nodes are connected by the information being passed from one layer to the other so multiplying all of the nodes in the output from later 1 in layer 2 is 'connecting' them.\n",
    "\n",
    "You will have to complete the constructor for this class as well as layer 2 within this model for the forward and backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb5811-2c34-4772-b47d-5189e6c6fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SimpleDNN_NP architecture\n",
    "class SimpleDNN_NP:\n",
    "    def __init__(self, hidden_size):\n",
    "        # We need to have a constructor here as our model has some parameters which need to be initialized & stored\n",
    "\n",
    "        # This example only works for input and output elements of dimension 1\n",
    "        # Formally you can extend this to work with batches with elements larger than 1\n",
    "        # But that is not the focus of this example\n",
    "        input_size = 1\n",
    "        output_size = 1\n",
    "\n",
    "        # We want to build a simple 3 layer network\n",
    "        # The first layer has input_size nodes, the second hidden_size nodes, and the third output_size nodes\n",
    "        # Weights need to be constructed to connect the nodes in each layer\n",
    "        # Weights should be initialized randomly, biases are initialized to zero\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) # Weights Layer 1\n",
    "        self.b1 = np.zeros(hidden_size) # Biases Layer 1\n",
    "        self.W2 = ## FINISH_ME ##\n",
    "        self.b2 = ## FINISH_ME ##\n",
    "        self.W3 = ## FINISH_ME ##\n",
    "        self.b3 = ## FINISH_ME ##\n",
    "\n",
    "\n",
    "        # These are used for tracking the model states during the Forward Pass\n",
    "        self.z1 = self.z2 = self.z3 = None # pre-activation values\n",
    "        self.a1 = self.a2 = self.a3 = None # activation values\n",
    "\n",
    "        # These are used for tracking the model states during the Backward Pass\n",
    "        self.gradient_W3 = self.gradient_W2 = self.gradient_W1 = None # gradients of weights\n",
    "        self.gradient_b3 = self.gradient_b2 = self.gradient_b1 = None # gradients of biases\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This method will be the 'evaluation' of the model\n",
    "\n",
    "        # Forward pass through the network\n",
    "\n",
    "        # We expect to have our data batched into dimension: ( ?, 1)\n",
    "        # This means that we multiply each element within the input so:\n",
    "        #     (?, 1) * (1, hidden_dim) = (?, hidden_dim) + b\n",
    "        # From this we then get:\n",
    "        #     Activation((?, hidden_dim)+b) * ((hidden_dim, hidden_dim)+b2) = (?, hidden_dim)\n",
    "        # Finally:\n",
    "        #     Acivation((?, hidden_dim)+b2) * (hidden_dim, 1) = (?, 1)\n",
    "\n",
    "        # From our lecture we know that the forward pass is just a series of matrix multiplications and activations\n",
    "        # In order to perform back-propagation we need to store the intermediate values of the forward pass\n",
    "        # This means we need to store the pre-activation (z) and activation (a) values of each layer\n",
    "\n",
    "        # Pass the input data x through the first layer of the network\n",
    "        # Apply Weights and biases, Layer1\n",
    "        self.z1 = np.matmul(x, self.W1) + self.b1 # pre-activation of layer1\n",
    "        self.a1 = np.tanh(self.z1)  # activation function gives activated layer1 output\n",
    "\n",
    "        # Pass the output from later 1 through the second layer of the network\n",
    "        # Apply Weights and biases, Layer2\n",
    "\n",
    "        self.z2 = ## FINISH_ME ##\n",
    "        self.a2 = ## FINISH_ME ##\n",
    "\n",
    "        # Pass the output from layer 2 through the third layer of the network\n",
    "        # Apply Weights and biases, Layer3\n",
    "        self.z3 = np.matmul(self.a2, self.W3) + self.b3 # pre-activation of layer3\n",
    "        # No activation function for Layer3\n",
    "\n",
    "        # 'formally' some models need to 'project' their internal state to the output\n",
    "        # For us this is done in Layer3\n",
    "        y_pred = self.z3\n",
    "        return y_pred\n",
    "\n",
    "    def backward(self, x, loss_prime):\n",
    "        # Backward pass (gradient descent)\n",
    "\n",
    "        # The backward pass is the 'reverse' of the forward pass\n",
    "        \n",
    "        # Initial gradient for the whole graph is given as the derivative of the loss function\n",
    "        # aka. loss_prime\n",
    "\n",
    "        # Going from Loss back through layer3\n",
    "        # Gradients for Layer3  = dy/dz3 * dL/dy = Layer2_output * Layer3_gradient = a2 * loss_prime\n",
    "        self.gradient_W3 = np.matmul(self.a2.T, loss_prime) # No activation for W3 so just calculate graph gradient here\n",
    "        self.gradient_b3 = np.sum(loss_prime, axis=0)       # Calculate the bias gradient here db3 = dL/dz3 = loss_prime\n",
    "\n",
    "        # Step from Layer3 -> layer2\n",
    "        # Stepping back from Layer3 to Layer2 => Undo the effect of W3 on the gradient and the activation on Layer2 output\n",
    "        gradient_a2 = np.matmul(loss_prime, self.W3.T)          # 'Undo' the effect of W3 on the gradient\n",
    "        gradient_z2 = gradient_a2 * (1 - np.tanh(self.a2) ** 2) # 'Undo' the effect of Activation on Layer2 output\n",
    "\n",
    "        # Gradients for Layer2\n",
    "        # Gradients for Layer2 = a1 * gradient_z2 = a1 * dL/dz3 * dz3/da2 * da2/dz2 = a1 * loss_prime * W3 * (1 - tanh(a2)^2)\n",
    "\n",
    "        self.gradient_W2 = ## FINISH_ME ##                   # Apply Gradient at layer2 to graph for full gradient\n",
    "        self.gradient_b2 = ## FINISH_ME ##                   # Calculate bias gradient here db2 = dL/dz2 = loss_prime * W3 * (1 - tanh(a2)^2)\n",
    "\n",
    "        # Step from Layer2 -> Layer1\n",
    "        # Stepping back from Layer2 to Layer1 => Undo the effect of W2 on the gradient and the activation on Layer1 output\n",
    "        gradient_a1 = ## FINISH_ME ##                        # 'Undo' the effect of W2 on the gradient\n",
    "        gradient_z1 = ## FINISH_ME ##                        # 'Undo' the effecr of the Activation on Layer1 output\n",
    "\n",
    "        # Gradients for Layer1\n",
    "        # Gradients for Layer1 = x * gradient_z1 = x * dL/dz3 * dz3/da2 * da2/dz2 * dz2/da1 * da1/dz1 = x * loss_prime * W3 * (1 - tanh(a2)^2) * W2 * (1 - tanh(a1)^2)\n",
    "        self.gradient_W1 = np.matmul(x.T, gradient_z1) # Apply Gradient at Layer1 onto the input data\n",
    "        self.gradient_b1 = np.sum(gradient_z1, axis=0) # Calculate the bias gradient here\n",
    "\n",
    "        # Reached the end of the graph\n",
    "        # Store the gradients for the weights and biases\n",
    "\n",
    "    def calculate_loss(self, y, y_pred):\n",
    "        # Calculate the loss of the whole 'graph'\n",
    "\n",
    "        # For simplicity\n",
    "        diff = y_pred - y\n",
    "\n",
    "        # Loss for whole graph is (y_pred-y)^2/2\n",
    "        loss = diff**2 / 2.0\n",
    "\n",
    "        # Initial gradient for whole graph\n",
    "        # dL/dy = 2.0 * diff / 2.0\n",
    "        loss_prime = diff # derivative of loss w.r.t. prediction\n",
    "\n",
    "        return loss, loss_prime\n",
    "\n",
    "    def optimize(self, learning_rate):\n",
    "        # Perform the optimization step of the training loop, i.e. update weights\n",
    "        # Very similar to plain Stochastic Gradient Descent (SGD)\n",
    "        \n",
    "        # Update weights and biases using gradients of each component and LR\n",
    "        self.W1 -= learning_rate * self.gradient_W1\n",
    "        self.b1 -= learning_rate * self.gradient_b1\n",
    "\n",
    "        self.W2 -= ## FINISH_ME ##\n",
    "        self.b2 -= ## FINISH_ME ##\n",
    "\n",
    "        self.W3 -= learning_rate * self.gradient_W3\n",
    "        self.b3 -= learning_rate * self.gradient_b3\n",
    "\n",
    "        # For completeness, but shouldn't matter\n",
    "        self.a1 = self.a2 = self.a3 = None\n",
    "        self.z1 = self.z2 = self.z3 = None\n",
    "        self.gradient_W3 = self.gradient_W2 = self.gradient_W1 = None\n",
    "        self.gradient_b3 = self.gradient_b2 = self.gradient_b1 = None  \n",
    "    \n",
    "    def train(self, x, y, epochs, learning_rate):\n",
    "\n",
    "        # History to store the evolution of the loss function vs epoch\n",
    "        loss_history = []\n",
    "\n",
    "        # Loop through x Epochs\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            ## Formally there should be some batching of data that is done here\n",
    "            ## This example explicitly evaluates the whole dataset in each forward/backward pass\n",
    "            ## For training on a simple sinusouid this is OK\n",
    "            ## For training on 'real data' this approach will kill your performance\n",
    "            \n",
    "            # Take a forward step through our model\n",
    "            y_pred = self.forward(x)\n",
    "\n",
    "            loss, loss_prime = self.calculate_loss(y, y_pred)\n",
    "\n",
    "            # Now take a backward step through our model\n",
    "            self.backward(x, loss_prime)\n",
    "\n",
    "            # Now update our weights based on the gradients at each point in the graph\n",
    "            self.optimize(learning_rate)\n",
    "\n",
    "            avg_loss = ## FINISH_ME ##\n",
    "            # Some code to give output during training \n",
    "            if epoch%5000 == 0:\n",
    "                print(f\"epoch: {epoch}, loss: {avg_loss}\")\n",
    "\n",
    "            loss_history.append(avg_loss)\n",
    "\n",
    "        return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1067661-8f6a-4f0a-a842-cd470bf93a9c",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 1 - Construct our input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa2558-0264-46d7-87a2-4a66f37d8e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sinusoidal data\n",
    "timesteps = 100  # number of timesteps in the data\n",
    "\n",
    "# It's up to you to populate x with an ndarray of 'timesteps' linearly spaced samples between 0 and 2*np.pi using the np.linspace function\n",
    "x = np.linspace( ## FINISH_ME ## )\n",
    "print(f\"x type: {type(x)}\")\n",
    "print(f\"x shape: {x.shape}\")\n",
    "# Now we want to fill y with the sin of the above parameters giving us 1 full sinusoid waveform\n",
    "# If you've constructed x correctly you can just use np.sin(x)\n",
    "y = ## FINISH_ME ##\n",
    "print(f\"y type: {type(y)}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3fbb5-d3b2-47b2-8410-9e128fbf4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x and y for training (our model is explicitly designed to take inputs of (1,) in shape and make an output the same)\n",
    "x_train = x.reshape(-1, 1)\n",
    "y_train = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33966ec0-a08c-4a14-8b16-36cb5f89ea2d",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 1 - Construct Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3041aa-1bfa-43ea-9aad-ef66b4e96449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the DNN\n",
    "hidden_size = 10  # number of neurons in the hidden layers which we will pass to our model\n",
    "\n",
    "# Initialize and train the model\n",
    "part1_model = SimpleDNN_NP(hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831db4bf-af67-4700-b982-c7bc76b727d6",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 1 - Make prediction using our un-trained model\n",
    "\n",
    "This step is important for 2 reasons:\n",
    "\n",
    "1) It shows that the forward part of our model and the constructor appear consistent and run correctly. (This reduces the possible code errors in training)\n",
    "2) It allows us to visualize our dataset and compare what the un-trained model evaluates to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ac35c-82bc-4ea5-b564-c0e29bf5e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions before training this is achieved by calling model.forward(data)\n",
    "y_pred_before = part1_model.forward( ## FINISH_ME ## )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b415990-f6b6-4271-8388-031aea78aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results before training\n",
    "# We can use plt.scatter to construct a scatter plot of x,y coordinates with a label\n",
    "plt.scatter(x, y, label='Original')\n",
    "plt.scatter( ## FINISH_ME ## , label='Model Prediction (Before Training)')\n",
    "\n",
    "# Define some important labels that make the graph mean something\n",
    "plt.title(\"DNN Predictions vs Original Data (Before Training)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "# Lets add a Legend and plot our graph\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dcced3-ff25-4742-a3b5-6b185399089d",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 1 - Now lets train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c06071f-00a3-4a7a-af0a-81ea00fae028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Our model is trained by calling model.train( ... )\n",
    "\n",
    "# The parameters we need to pass to this model are:\n",
    "#    input_data, input_labels, how-long-to-train, learning-rate\n",
    "#    x_train,    y_train,      epochs,            learning-rate\n",
    "\n",
    "history = part1_model.train( ## FINISH_ME ## )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd8a850-6ed4-4e75-84d5-d692feeac513",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 1 - Let's plot the loss function through this training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ad609-3b66-4e27-bca8-668a248b4852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use matplotlib to plot the loss function vs epoch\n",
    "# This can be achieved simply by using the plt.plot( ... ) method which takes a list of values to plot\n",
    "# We can do this because the number of epochs is just an interating list so need to construct this for plotting a scatter plot\n",
    "\n",
    "plt.plot(history, label='Average Model Loss')\n",
    "\n",
    "# Add labels, legend, make log and plot\n",
    "plt.title(\"DNN Model Loss vs Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765e3408-1a80-4886-9876-764f83a7922b",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 1 - Make prediction after model has been trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff03a34-cd5e-4776-b2eb-ca4b5beb5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions after training\n",
    "# As before lets make a prediction but with our trained model\n",
    "y_pred_after = part1_model.forward(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81b8b8-3940-4dc3-a6a9-37573920695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results before training\n",
    "plt.scatter(x, y, label='Original')\n",
    "plt.scatter( ## FINISH_ME ##, label='Model Prediction (After Training)')\n",
    "plt.title(\"DNN Predictions vs Original Data (After Training)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a96a3-0d4f-4b28-a3df-ccdd2d568646",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de19e9b-c7ed-4114-b0e4-19c57f0e3710",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Part 2 - PyTorch DNN\n",
    "This part of the notebook walks through building a DNN using the PyTorch API\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c79e8b-9e3d-4d18-9380-1c404fd94035",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 2 - Imports and Globals\n",
    "\n",
    "Here we want to make sure we have the relavent parts of the PyTorch framework loaded for us to use later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c507657-dfea-49c0-a3ec-16f2756a953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f358db9-5532-432e-bac6-308aac45eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch Supports 'accelerator' devices such as CUDA on Linux and MPS on MacOS\n",
    "\n",
    "# If you're lucky enough to have access to this, lets take advantage of it\n",
    "\n",
    "# Select the best available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")   # Apple Metal (MPS)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # CPU fallback\n",
    "\n",
    "# Report the device that we're using\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0393d1c4-ea9f-477b-852a-fb2263625e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a connection of globals needed to make everything re-producible\n",
    "\n",
    "torch.manual_seed(_FIXED_SEED)  # PyTorch CPU\n",
    "\n",
    "# Ensure reproducibility on Metal (MPS)\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(_FIXED_SEED)  # Fix seed for MPS backend\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(_FIXED_SEED)  # PyTorch GPU (if used)\n",
    "    torch.cuda.manual_seed_all(_FIXED_SEED)  # If using multi-GPU\n",
    "\n",
    "    # Ensure deterministic behavior in CUDA operations (if available)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # Disable auto-tuner for determinism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9901bc4e-0d69-4264-bb92-ee87770e5ba8",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 2 - Convert the NumPy dataset to use with PyTorch\n",
    "\n",
    "PyTorch uses objects called \"Tensors\" for passing around data.\n",
    "\n",
    "PyTorch also expects these Tensor objects to be sent to a device if the data needs to be there.\n",
    "\n",
    "e.g. before a model can run on a GPU we need to send the model and the data to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d63cd45-113f-499c-a7d5-a11a288d6c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "# We're converting the shape here because PyTorch will helpfully reduce the extra 1-dim from our dataset but we want it\n",
    "#\n",
    "# Why do you think we're using torch floats here?\n",
    "#\n",
    "X_tensor = torch.tensor(x_train, dtype=torch.float32).reshape(-1,1).to(device)\n",
    "Y_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1,1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa587e47-300e-4e6a-bd8b-46c962377bb9",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 2 - Construct a simple DNN in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1417726a-7b77-44d0-a899-cc45a9c063d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple DNN model\n",
    "class SimpleDNN(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        # This is our model's constructor\n",
    "\n",
    "        # This model is inheriting from existing classes in PyTorch\n",
    "        # This is needed for inheritance to work properly\n",
    "        super(SimpleDNN, self).__init__()\n",
    "\n",
    "        # The object self.model will contain the important part of the model in PyTorch\n",
    "\n",
    "        ## The nn.Sequantial model allows us to simply pass a list of layers that we want PyTorch to construct\n",
    "        ## Every single layer in a DNN(MLP) in a nn.Linear class in PyTorch which needs to know it's input and output dim\n",
    "        ## Between each layer (but not at the output!) we need to add an activation\n",
    "        ## As above we're going to use the Tanh function which is accessed via nn.Tanh()\n",
    "\n",
    "        ## We want a 3 layer DNN which has an input dim of 1, (\"hidden_size\", \"hidden_size\") middle layer\n",
    "        ## and an output dim of 1 \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            ## FINISH_ME ##\n",
    "            ## FINISH_ME ##\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    ## We also want a forward pass method to know how to evaluate our model\n",
    "    def forward(self, x):\n",
    "        # This simply calls the internal self.model object\n",
    "        return self.model(x)\n",
    "\n",
    "    ## We don't need to explicitly define a backwards method, we get that free from PyTorch :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51abbb6f-5e20-4f37-9e6b-2b5a270df5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our model and pass it to any accelerator we have access to\n",
    "part2_model = SimpleDNN(hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61102400-fb60-48bd-95f0-ef0e14e04058",
   "metadata": {},
   "source": [
    "***\n",
    "##  Part 2 - Make predictions using our pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35f7636-6a52-40f3-98c0-634ad06bbb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "\n",
    "## Make sure the model is in evaluate mode\n",
    "part2_model.eval()\n",
    "\n",
    "## Take a prediction using\n",
    "prediction_Tensor = part2_model(X_tensor)\n",
    "\n",
    "print(f\"Prediction type: {type(prediction_Tensor)}\")\n",
    "print(f\"Input Data Shape: {X_tensor.shape}\")\n",
    "print(f\"Output Data Shape: {prediction_Tensor.shape}\")\n",
    "\n",
    "## Make sure that our prediction has been copied back to the CPU\n",
    "## Then convert it to numpy so we can use it elsewhwre\n",
    "predictions = prediction_Tensor.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02fdd23-bfec-4ba8-bba7-40705fd85524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "## Construct the canvas\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "## As with the NumPY model use plt.scatter\n",
    "plt.scatter(x_train, y_train, label='True')\n",
    "\n",
    "## We can also plt.plot to plot \"scatter-like\" data\n",
    "plt.plot( ## FINISH_ME ##, label='Predicted', color='red')\n",
    "\n",
    "## Make the graph so we can see it\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f45d8-44bf-4ce2-a590-30cd662701ca",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 2 - Construct some other objects needed to work with the PyTorch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1684737c-88aa-48fe-b138-f0d860bb60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "\n",
    "## We need some way of defining the loss function\n",
    "## For our example we will use the nn.L1Loss class as our criterion\n",
    "## This class gives |y-y_pred| which is good for our example\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "## We also want to use a built-in optimizer for PyTorch\n",
    "## There are many different possible optimizers, but we want to use the optim.SGD class\n",
    "## The optimizer needs to know 2 things:\n",
    "##   what it's optimizing  - part2_model.parameters()\n",
    "##   how fast to train     - learning-rate (lr)\n",
    "optimizer = optim.SGD(part2_model.parameters(), lr=0.001)\n",
    "\n",
    "## As with training our NumPy class we want to keep track of our Loss function values\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f08630e-fee6-468e-ab75-023be47130a1",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 2 - Train our PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9637b4-4b80-4751-83c9-d1ae3b35aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 75000\n",
    "\n",
    "## Iterate through all of the epochs\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    ## If we were using batches we would loop through batches here\n",
    "    ## Put the model into 'training' mode\n",
    "    part2_model.train()\n",
    "\n",
    "    ## Re-Set the optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    ## Evaluate a Forward Pass of our model\n",
    "    outputs = part2_model(X_tensor)\n",
    "\n",
    "    ## Calculate our loss based on the criterion\n",
    "    ## (Outputs - Truth) with the criterion function\n",
    "    loss =  ## FINISH_ME ##\n",
    "\n",
    "    ## Evaluate the Backward Pass of our model\n",
    "    loss.backward()\n",
    "    ## There we go, nothing else needed :)\n",
    "\n",
    "    ## Now use the Optimizer to tune our model based on our recent loss\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    losses.append(loss.item())  # Store loss for plotting\n",
    "\n",
    "    ## Report how far through the training we are\n",
    "    if epoch % 5000 == 0:\n",
    "        print(f\"Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755bebdd-4fd8-4bce-8a89-997533883f4c",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 2 - Evaluate our model and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68057897-8b3b-4cd7-8baa-9d64f7bf74a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "# As with above put the model in evaluate mode\n",
    "part2_model.eval()\n",
    "\n",
    "## As with above we want to evaluate our model using our dataset\n",
    "## Then we need to pass it back to the CPU then NumPY\n",
    "predictions = part2_model(X_tensor)\n",
    "predictions = predictions.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ea452-d0be-4956-8de1-a55c6e8b4ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "## Construct the Canvas\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "## Plot the input data and the Model 'Prediction' (Output)\n",
    "plt.scatter(x_train, y_train, label='True', alpha=0.6)\n",
    "plt.scatter( ## FINISH_ME ##, label='Predicted', color='red')\n",
    "\n",
    "## Plot the Graph\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc57618-f5c3-40c0-8bcc-8c399cbcfab4",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 2 - Lets examine our loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b661660-ebe1-48df-8c30-cbfe2a781c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "## Construct the Canvas\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "## Plot the losses collected during training\n",
    "plt.plot(losses, label=\"Loss vs. Epoch\", color=\"blue\")\n",
    "\n",
    "## Add labels and Plot\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd56870b-b1a3-49ed-aa96-5eb07aeb3ae4",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Part 3 - Building a Classifier\n",
    "This part of the notebook walks through building and training a Classifier using the NumPy API\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85870750-40d0-45db-82f1-93a5e31cc641",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Load the modules needed to Build, Train and examine a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5d89f6-1afe-4357-b820-bafb8039456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4001cf38-fba2-47de-b84b-2ffd76febee3",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Set some Globals to make everything reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe11c99-3f51-4954-adef-40636912d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set manual seed for reproducibility\n",
    "# This is a connection of globals needed to make everything re-producible\n",
    "\n",
    "torch.manual_seed(_FIXED_SEED)  # PyTorch CPU\n",
    "\n",
    "# Ensure reproducibility on Metal (MPS)\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(_FIXED_SEED)  # Fix seed for MPS backend\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(_FIXED_SEED)  # PyTorch GPU (if used)\n",
    "    torch.cuda.manual_seed_all(_FIXED_SEED)  # If using multi-GPU\n",
    "\n",
    "    # Ensure deterministic behavior in CUDA operations (if available)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # Disable auto-tuner for determinism\n",
    "\n",
    "np.random.seed(_FIXED_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89416ddf-42dc-413f-aec6-97757d68f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As in Part 2 if you have a supported 'accelerator' it's nice to use it\n",
    "\n",
    "# Select the best available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")   # Apple Metal (MPS)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # CPU fallback\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7763f0-5c87-4528-8671-3fb613440512",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ba0555-28b7-4718-bdbb-12270e685329",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Loading the Dataset so that we can use it to build a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f83d9df-5f11-400a-9d47-548857dd340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset and split into training and verification sets\n",
    "def load_mnist_as_numpy(train=True):\n",
    "    dataset = torchvision.datasets.MNIST(root=\"./data\", train=train, download=True)\n",
    "\n",
    "    # Convert images & labels to NumPy arrays\n",
    "    images = np.array([np.array(img, dtype=np.float32) for img, _ in dataset])\n",
    "    labels = np.array([label for _, label in dataset], dtype=np.int64)\n",
    "\n",
    "    # Normalize manually: Convert [0, 255] → [-1, 1]\n",
    "    images = (images / 127.5) - 1.0\n",
    "\n",
    "    # Reshape to (N, 28, 28) for experimenting\n",
    "    images = images.reshape(-1, 28, 28)\n",
    "\n",
    "    # Convert labels to one-hot encoding (equivalent to looking at `categorical`)\n",
    "    labels = np.eye(10)[labels]\n",
    "\n",
    "    return images, labels  # For test set (no split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa34f1a-5eda-4040-88cf-5e128cc56da5",
   "metadata": {},
   "source": [
    "### First Load the dataset\n",
    "\n",
    "This returns a numpy object which you can use to examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542eb7bc-00f2-4841-a21c-55951202999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "X_train, Y_train = load_mnist_as_numpy(train=True)\n",
    "X_test, Y_test = load_mnist_as_numpy(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a71e68-c3fe-483a-8c9a-b2ac850d3b23",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Examine the dataset\n",
    "\n",
    "What is the size and shape of the data we're working with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd21901-fc6b-4382-affb-28a985b0dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X shape: {X_train.shape}\")\n",
    "print(f\"Y shape: {Y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a1dba-b962-49d2-9b5f-836da9fa0507",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_tight_layout(True)\n",
    "## Plot a single image from the dataset\n",
    "plt.imshow(## FINISH_ME ##, cmap='gray')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114da0d-edcb-4dbd-a69a-61f0f71d24f6",
   "metadata": {},
   "source": [
    "### Now we need to convert this to Tensors for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc0b41-6e35-4b83-9898-772a114958aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "## Use the torch.from_numpy to construct Tensors from Numpy Arrarys\n",
    "X_train, Y_train = torch.from_numpy(X_train).to(device), torch.from_numpy(Y_train).float().to(device)\n",
    "X_test, Y_test = torch.from_numpy(X_test).to(device), torch.from_numpy(Y_test).float().to(device)\n",
    "\n",
    "# Create DataLoaders\n",
    "## Our dataset is constructed from the Data and Labels\n",
    "## We are chosing to use batches of 64 images in size with this model\n",
    "train_dataset = torch.utils.data.DataLoader(list(zip(X_train, Y_train)), batch_size=64, shuffle=True)\n",
    "test_dataset = torch.utils.data.DataLoader(list(zip(X_test, Y_test)), batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f84ba-e782-4905-aee3-3b20d535b4c9",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Build a Classifier DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f864e6-1b7e-4406-abd8-c78b6022d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model using PyTorch's `Sequential`\n",
    "## We just want to use the nn.Sequential directly here no wrapper classes\n",
    "part3_model = nn.Sequential(\n",
    "    nn.Flatten(),                # Used to make sure the data from each batch is a flat numerical array\n",
    "    nn.Linear(28 * 28, 128),     # Fully connected layer input -> 128 dim\n",
    "    nn.ReLU(),                   # Activation   (I like ReLU)\n",
    "    ## FINISH_ME ##              # Hidden layer (128 -> 64 dim)\n",
    "    ## FINISH_ME ##              # Activation   (I like ReLU)\n",
    "    nn.Linear(64, 10),           # Output layer (64-dim -> logits output)\n",
    "    nn.Sigmoid()                 # Final output\n",
    ")\n",
    "# Why do you think we're using a Sigmoid activator on this model but not for the earlier one?\n",
    "\n",
    "# Move model to device\n",
    "part3_model = part3_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677483f-466a-4fd3-a52f-45806d6b1d79",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Make Some predictions before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb36d02-28d4-454b-bf8f-edf5ed1ab02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to pass a single entry to our model for evaluation/prediction it needs to be in a (1, X) rather than just passing 1 element\n",
    "# This is the same as 'passing a batch of 1' to the model\n",
    "prediction = part3_model(X_train[0].view(1, -1))\n",
    "print(f\"Prediction Type: {type(prediction)}\")\n",
    "\n",
    "truth = Y_train[0].cpu().detach().numpy()\n",
    "prediction = prediction.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbead3-6bdd-42e2-813d-5ada6c35e83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the predictions\n",
    "fig = plt.figure()\n",
    "x=[_ for _ in range(len(prediction[0]))]\n",
    "plt.bar(x, prediction[0])\n",
    "plt.title('Prediction Distribution')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa83c39-68bc-4145-ad93-2b87aa27b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Truth: {np.argmax( ## FINISH_ME ##\n",
    "print(f\"Prediction: {np.argmax( ## FINISH_ME ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef00b33-21d4-49fb-8e26-387f77ac3585",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Define some objects needed for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb37b5-ad14-437b-b16e-7a27dac3c43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 10\n",
    "\n",
    "# We want to track losses and the accuracy of our model during training\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # No need for one-hot encoding in loss function\n",
    "optimizer = optim.SGD(part3_model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeed624-515d-45b7-99d0-80caacf28c4c",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Now train our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4916b4-a57f-4e84-b48b-5d47d25fa21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "## Loop through n-epochs\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    ## Reset some counters we're going to use\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    ## Looping through all batches in our dataset here\n",
    "    for images, labels in train_dataset:\n",
    "\n",
    "        ## Put our model into training mode\n",
    "        part3_model.train()\n",
    "\n",
    "        ## Make sure that the data we're interested in evaluating is on the correct device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        ## Reset our Optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ## Evaluate this batch of images with our model\n",
    "        outputs = part3_model(images)\n",
    "\n",
    "        ## Our model outputs an array of values, lets compare this to truth to get our loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        ## Evaluate the back-propagation of our model\n",
    "        loss.backward()\n",
    "\n",
    "        ## Optimize our model based on evaluating this batch\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        ## We can put the model back into non-training mode here \n",
    "        part3_model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            ## Add the loss from batch to the total loss from this epoch\n",
    "            ## .item() here returns the raw values no need to move off GPU\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            ## Calculate how many times the model evaluated correctly\n",
    "            ## .item() here returns the raw values no need to move off GPU\n",
    "            correct += (outputs.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
    "\n",
    "            ## What was the size of this batch? i.e. how many datapoints processed?\n",
    "            total += len(labels)\n",
    "\n",
    "    # Back into non-training mode\n",
    "    part3_model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        ## Calculate the average loss of the dataset over the whole epoch\n",
    "        avg_train_loss = ## FINISH_ME ##\n",
    "        ## Store the average loss per epoch\n",
    "        train_losses.append(avg_train_loss)\n",
    "        ## Calculate the average accuracy per epoch\n",
    "        train_accuracy = ## FINISH_ME ##\n",
    "        ## Store the average accuracy per epoch\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "    ## Report the average loss and Accuracy per epoch during fitting\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a3c530-f1ae-4ed5-84c3-dc41095ed6f3",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Lets examine our taining history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77947bb-91e1-4718-891a-007a6cf6525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and accuracy\n",
    "## Make a convas\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Training & Validation Loss\n",
    "## Mak\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot( ## FINISH_ME ##, label=\"Train Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot Training & Validation Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot( ## FINISH_ME ##, label=\"Train Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d54a2-894d-48c1-9bfb-464656d24e10",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3 - Estimate model accuracy\n",
    "\n",
    "Now use the test dataset to make an estimate as to how accurate the model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c079e-ecfe-4fb8-911f-4e96790f825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "test_prediction = part3_model(X_test.to(device))\n",
    "print(test_prediction.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71beb593-f91b-4f80-a0e7-14550c050c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct += (test_prediction.argmax(dim=1) == Y_test.argmax(dim=1)).sum()\n",
    "print(f\"Test Accuracy = {correct/len(test_prediction)*100:.4}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd80013-2f56-4b6e-a256-cd2a46087f35",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Part 4 - Projecting a DNN beyond the training window\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089dc3f4-607b-4994-92a1-3c84a4cdf961",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 4 - Now generate a new dataset\n",
    "\n",
    "This dataset needs to be 5x as long with 5x as much data and containing 5 waveforms compared to Part1 and Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c900aed-187b-4c6d-b842-f48410dd8391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sinusoidal data\n",
    "timesteps = ## FINISH_ME ##\n",
    "x = ## FINISH_ME ##\n",
    "y = ## FINISH_ME ##\n",
    "# Reshape x and y for training (our model is explicitly designed to take inputs of (1,) in shape and make an output the same)\n",
    "x_beyond = x.reshape(-1, 1)\n",
    "y_beyond = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe813a-ce9a-4b1d-b703-c34ef9100d3a",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 4 - Now evaluate our NumPY DNN and plot what we see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eed3e7-e01f-4117-882d-c2a4fd5a037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "predictions = part1_model.forward(x_beyond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a35a5-21d7-44e3-90aa-a62e8efc2c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_beyond, y_beyond, label='True', alpha=0.6)\n",
    "plt.scatter( ## FINISH_ME ## , label='Predicted', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21db1e62-d2a2-40f0-8a85-3a1c28306f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(x_beyond, dtype=torch.float32).reshape(-1,1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afaec8e-06be-430b-b2fd-a52b0d2ef097",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 4 - Now evaluate our PyTorch DNN and plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d439b42-1192-42b7-8815-5e33854a2bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "part2_model.eval()\n",
    "predictions = part2_model(X_tensor).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff2a3a5-36af-4cf6-967d-a31d7f138fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_beyond, y_beyond, label='True', alpha=0.6)\n",
    "plt.scatter( ## FINISH_ME ##, label='Predicted', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3128c-6975-48eb-a155-bb669d0c46fe",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 4 - Do the DNN/MLP models extend as you expected it to?\n",
    "\n",
    "What do you see and can you think why this is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba8074e-afb1-4e0c-a13a-a9f879d737b8",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "`## FINISH_ME ##`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cad753-66ea-464b-892f-30908ff4f6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
