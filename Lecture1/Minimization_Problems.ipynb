{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2ac514-dfe7-406e-8e2d-5da645a65b81",
   "metadata": {},
   "source": [
    "# 0 Minimizing using Gradient Descent\n",
    "\n",
    "Simple minimization using Gradient Descent involves using knowledge of the instantaneous 1st derrivative only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b2f84-1a88-4287-8199-eb90f7b0b216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the function to be minimized\n",
    "def f(x):\n",
    "    return x**4 - 3*x**3 + 2  # Example function\n",
    "\n",
    "# Numerical approximation of the gradient (first derivative) using finite differences\n",
    "def numerical_gradient(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# Gradient descent approach (without Hessian)\n",
    "def gradient_descent_minimization(x0, step_size=0.1, tol=1e-6, max_iter=100):\n",
    "    x = x0  # Initial guess\n",
    "    for i in range(max_iter):\n",
    "        grad = numerical_gradient(f, x)\n",
    "\n",
    "        # Gradient-based update step\n",
    "        x_new = x - step_size * grad\n",
    "\n",
    "        print(f\"Iteration {i+1}: x = {x_new:.6f}, f(x) = {f(x_new):.6f}, grad = {grad:.6f}\")\n",
    "\n",
    "        # Check for convergence\n",
    "        if abs(x_new - x) < tol:\n",
    "            print(\"Convergence reached.\")\n",
    "            break\n",
    "        \n",
    "        x = x_new  # Update for the next iteration\n",
    "    return x\n",
    "\n",
    "# Initial guess\n",
    "x0 = 5\n",
    "\n",
    "# Perform minimization\n",
    "optimal_x = gradient_descent_minimization(x0, step_size=0.01)\n",
    "print(f\"Optimal solution: x = {optimal_x:.6f}, f(x) = {f(optimal_x):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444673de-75b6-4999-becb-4bc4f2694f85",
   "metadata": {},
   "source": [
    "## Varying the Search\n",
    "Now explore the impact of changing the step size from 0.1-0.001 on the search for a true minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a713d23-1d26-48f2-b1f6-19b257f629b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0751172f-8136-4ec8-89a0-c44313be2179",
   "metadata": {},
   "source": [
    "## Repeat the Search\n",
    "\n",
    "As an example to make sure you understand what is going on try mnimizing the function:\n",
    "\n",
    "```\n",
    "x^5-40x^3+23x^2+140=0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe002a41-defd-4643-ac4d-a755ea7b65e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34eff8c4-5224-4bc7-ae1b-202f8cf8f6ad",
   "metadata": {},
   "source": [
    "# 1 Minimization using Newton Raphson\n",
    "\n",
    "Newton Raphson makes use of knowledge of the 2nd derrivative to achieve the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77bda1-fd59-44ed-ab97-b03ee6933e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the function to be minimized\n",
    "def f(x):\n",
    "    return x**4 - 3*x**3 + 2  # Example function (no known derivative used)\n",
    "\n",
    "# Numerical approximation of the first derivative using finite differences\n",
    "def numerical_gradient(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# Numerical approximation of the second derivative using finite differences\n",
    "def numerical_hessian(f, x, h=1e-5):\n",
    "    return (f(x + h) - 2*f(x) + f(x - h)) / (h**2)\n",
    "\n",
    "# Newton-Raphson minimization with numerical derivatives\n",
    "def newton_raphson_minimization_numerical(x0, step_size=0.1, tol=1e-6, max_iter=100):\n",
    "    x = x0  # Initial guess\n",
    "    for i in range(max_iter):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        hessian = numerical_hessian(f, x)\n",
    "\n",
    "        if abs(hessian) < 1e-10:\n",
    "            print(\"Hessian is too small, stopping to avoid division by zero.\")\n",
    "            break\n",
    "\n",
    "        # Newton-Raphson update step with step size\n",
    "        x_new = x - step_size * (grad / hessian)\n",
    "\n",
    "        print(f\"Iteration {i+1}: x = {x_new:.6f}, f(x) = {f(x_new):.6f}, grad = {grad:.6f}, hessian = {hessian:.6f}\")\n",
    "\n",
    "        # Check for convergence\n",
    "        if abs(x_new - x) < tol:\n",
    "            print(\"Convergence reached.\")\n",
    "            break\n",
    "\n",
    "        x = x_new  # Update for the next iteration\n",
    "    return x\n",
    "\n",
    "# Initial guess\n",
    "x0 = 10\n",
    "\n",
    "# Perform minimization\n",
    "optimal_x = newton_raphson_minimization_numerical(x0, step_size=0.01)\n",
    "print(f\"Optimal solution: x = {optimal_x:.6f}, f(x) = {f(optimal_x):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6726283-fbbf-4394-8f00-99fa2321a09e",
   "metadata": {},
   "source": [
    "## Exploring the fit stability\n",
    "\n",
    "For this example we want to explore the fit stability.\n",
    "\n",
    "Change the starting value of the fit between 1 and 10 and change the step size between 0.1 and 0.01 and explore the final fit results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c08e17-2807-439d-a289-e08b20d0771e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9487cb68-d995-412c-baec-146e335b3cb3",
   "metadata": {},
   "source": [
    "## Repeat the Search\n",
    "\n",
    "As an example to make sure you understand what is going on try mnimizing the function:\n",
    "\n",
    "```\n",
    "x^3-20x^2+124=0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35015b-c16b-4669-afe8-5e422a5bce92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08502822-ef77-48bd-a98b-3751ac358911",
   "metadata": {},
   "source": [
    "# 2 Minimization using SciPy\n",
    "\n",
    "The following gives a demonstration of how to perfom a minimization of a simple function using scipy optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586f6d4c-d781-4848-ae5d-2769f5a7f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the objective function (a simple quadratic function)\n",
    "def objective(x):\n",
    "    return x[0]**2 + x[1]**2 + 5*x[0] + 3*x[1] + 7\n",
    "\n",
    "# Callback function to track progress\n",
    "iteration = 0  # Global variable to track iterations\n",
    "\n",
    "def callback(xk):\n",
    "    global iteration\n",
    "    iteration += 1\n",
    "    print(f\"Iteration {iteration}: x = {xk}, f(x) = {objective(xk):.4f}\")\n",
    "\n",
    "# Initial guess\n",
    "x0 = np.array([3.7, -4.3])\n",
    "\n",
    "# Perform minimization with callback\n",
    "result = minimize(objective, x0, method='BFGS', callback=callback)\n",
    "\n",
    "# Display final results\n",
    "print(\"\\nOptimization complete!\")\n",
    "print(\"Optimal values of x:\", result.x)\n",
    "print(\"Minimum function value:\", result.fun)\n",
    "\n",
    "# Visualization\n",
    "x_vals = np.linspace(-5, 5, 400)\n",
    "y_vals = np.linspace(-5, 5, 400)\n",
    "X, Y = np.meshgrid(x_vals, y_vals)\n",
    "Z = objective([X, Y])\n",
    "\n",
    "plt.contour(X, Y, Z, levels=50, cmap='viridis')\n",
    "plt.scatter(result.x[0], result.x[1], color='red', marker='o', label='Minimum')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Optimization Path')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e5930-258c-4f80-9db1-1e3c5497e551",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "I would now like you to write the appropriate code to minimize and vizualize the minimum for the following functions:\n",
    "\n",
    "```\n",
    "x^2 + 3x - 20 = 0\n",
    "\n",
    "x^2 + y^2 - 10y + x = 0\n",
    "\n",
    "x^2 + 5x + y + 25 = 0 \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a510f7de-ac52-4759-9bca-ea01b0bff419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d780a68b-7052-4865-80d4-15e6882bd9bb",
   "metadata": {},
   "source": [
    "\n",
    "# 3 Fitting over a finite range\n",
    "\n",
    "### Problems when fitting (an interlude)\n",
    "\n",
    "When fitting to a dataset we trust that the algorithm that we're using will find the correct minima and that the fit hasn't become pronned to some problem such as vanishing gradients.\n",
    "\n",
    "Here we're going to demonstrate a quick example of problems that you can hit with a fit failing to converge.\n",
    "\n",
    "There are many tools/checks in place in most modern libraries to help fix common problems, but we will explicitly use the `SLSQP` method here which allows us to look at narrow ranges of a full function.\n",
    "\n",
    "`result = minimize(objective, pt, method='SLSQP', jac=derivative, bounds=[(...),])`\n",
    "\n",
    "The objective function of this fit has been provided it is the equation of the form `y=x^4-5x^2+0.1x` .\n",
    "\n",
    "This function has 3 points where the gradient is zero and has 2 minima, a global minima and a local minima.\n",
    "\n",
    "## 3.1 Provide the derivative function for this 1D method (this is the Jacobian)\n",
    "\n",
    "This is just a case of returning the correct value from the derivative method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba1c62-5985-4b88-add5-eb4beb41bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def objective(x):\n",
    "  return x[0]**4 - 5*x[0]**2 + 0.1*x[0]\n",
    "\n",
    "# Part 1 complete the derivative function\n",
    "def derivative(x):\n",
    "  return ## FINISH ME ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd31db3-9435-42d9-adc6-e2bd7544ddaa",
   "metadata": {},
   "source": [
    "## 3.2 Run the minimize method over given ranges and identify the 3 'stable' points of the function\n",
    "\n",
    "Run the minimize method over these 3 ranges:\n",
    "\n",
    "| <p align='left'> Ranges |\n",
    "| :--- |\n",
    "| ` (0.1, 5.0)` |\n",
    "| ` (0.0, 5.0)`  |\n",
    "| `(-5.0, 5.0)` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ccfd75-beed-49fd-8806-893cf2bb509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = (5.,)\n",
    "ranges = [(0.1, 5.0),  (0.0, 5.0),  (-5.0, 5.0)]\n",
    "\n",
    "# Part 2 perform 3 minimizations\n",
    "result_1 = minimize( # FINSIH ME #\n",
    "result_2 = minimize( # FINISH ME #\n",
    "result_3 = minimize( # FINISH ME #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11982c4-665d-43e1-ade2-309bcfd4710a",
   "metadata": {},
   "source": [
    "## 3.3 Identify the true minima of the function\n",
    "\n",
    "##### Hint:\n",
    "    \n",
    "Running `scipy.minimize` returns an object containing the fit result and a bunch of extra output.\n",
    "The value of `x` from a 'minimize' after it has finished running can be accessed via `result['x']` and the function value can be accessed via `result['fun']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6bcc47-9e6b-4c72-8ef3-225d6fb932ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Minimum 1: {}'.format(result_1['x']))\n",
    "print('Minimum 2: {}'.format(result_2['x']))\n",
    "print('Minimum 3: {}'.format(result_3['x']))\n",
    "\n",
    "print('Result 1: {}'.format(result_1['fun']))\n",
    "print('Result 2: {}'.format(result_2['fun']))\n",
    "print('Result 3: {}'.format(result_3['fun']))\n",
    "\n",
    "# Part 3 identify the true minima\n",
    "print('Best Result is: ' ) # FINISH ME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e02b69-1e5b-4107-94f3-1fa577783dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AGQenv] *",
   "language": "python",
   "name": "conda-env-AGQenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
