{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3bdd231-a289-42d0-b2ee-a2270bbe576d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CNN Playbook\n",
    "\n",
    "This notebook will introduce CNN building atop of DNN nodes and will examine how they work.\n",
    "\n",
    "Then after doing this we will build an auto-encoder for the mnist numerical dataset and examine the different aspects of this dataset up-close.\n",
    "\n",
    "## Part 1 2D Sobel filters\n",
    "\n",
    "This section is about building the 2D Sobel filters and applying them to an input image\n",
    "\n",
    "## Part 2 PyTorch 2D CNN filters\n",
    "\n",
    "This section covers building \n",
    "\n",
    "## Part 3 Building a VAE model\n",
    "\n",
    "This section is about Building a VAE model using the PyTorch API\n",
    "\n",
    "## Part 4 Training our VAE with Test/Validate\n",
    "\n",
    "This section is about Training the VAE model, again using PyTorch, but now with a validation dataset\n",
    "\n",
    "## Part 5 Load/Save model\n",
    "\n",
    "This section introduces the load/save model\n",
    "\n",
    "## Part 6 Examining the VAE Latent-Space\n",
    "\n",
    "This section is about taking the trained VAE model and examining the \n",
    "\n",
    "## Bonus 7 What is required to train over the CIFAR10 dataset\n",
    "\n",
    "If you have completed building a VAE model which can be trained over the B&W mnist numerical dataset.\n",
    "A common \"next step up\" is to use the CIFAR10 or CIFAR100 which contain 3x32x32 rgb images.\n",
    "\n",
    "The approach is the same as before, but what level of accuracy do you think you can achieve by training on this dataset?\n",
    "\n",
    "## Marking\n",
    "\n",
    "You will get marks for completeing the different tasks within this notebook:\n",
    "\n",
    "Any code expected for you to complete will contain `## FINISH_ME ##` indicating the code isn't expected to run until you have completed it.\n",
    "\n",
    "I would recommend tackling the playbook in order from Part1 -> Part2 -> Part3 -> Part4.\n",
    "\n",
    "\n",
    "| <p align='left'> Title                         | <p align='left'> Parts | <p align='left'> Number of marks |\n",
    "| ------------------------------------- | ----- | --- |\n",
    "| <p align='left'> 1. Construct 2D Sobel Filters and apply                   | <p align='left'>  2  | <p align='left'> 1 |\n",
    "| <p align='left'> 2. Constructing different sized 2D filters and examine output | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 3. Building the VAE model                                 | <p align='left'>  3  | <p align='left'> 3 |\n",
    "| <p align='left'> 4. Train the VAE model                                    | <p align='left'>  2  | <p align='left'> 2 |\n",
    "| <p align='left'> 5. Load/Save a model to disk                              | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 6. Examine the trained VAE latent space                   | <p align='left'>  1  | <p align='left'> 2 |\n",
    "| <p align='left'> **Bonus 1:** Training a VAE over the CIFAR10 dataset      | <p align='left'>  4  | <p align='left'> 1 |\n",
    "| <p align='left'> **Total** | | <p align='left'> max **10** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6bfb65-6ed8-4a93-b73e-2ef40b6b069d",
   "metadata": {},
   "source": [
    "# Part 0\n",
    "\n",
    "Load the requirements to run the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d55e2-35f8-4d01-8fa1-53ea371c8082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d61590-0a45-4a9c-bade-27ad9738ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility in Science is critial, in computing it's often just a convenience\n",
    "_FIXED_SEED=12345\n",
    "random.seed(_FIXED_SEED)\n",
    "np.random.seed(_FIXED_SEED)\n",
    "\n",
    "# This is a connection of globals needed to make everything re-producible\n",
    "torch.manual_seed(_FIXED_SEED)  # PyTorch CPU\n",
    "\n",
    "# Ensure reproducibility on Metal (MPS)\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(_FIXED_SEED)  # Fix seed for MPS backend\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(_FIXED_SEED)  # PyTorch GPU (if used)\n",
    "    torch.cuda.manual_seed_all(_FIXED_SEED)  # If using multi-GPU\n",
    "\n",
    "    # Ensure deterministic behavior in CUDA operations (if available)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # Disable auto-tuner for determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e37cab-0439-4ec7-8d43-05b470cff891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image using matplotlib's imread\n",
    "image_path = 'img.png'\n",
    "image = plt.imread(image_path)  # Reads image as (H, W, C) or (H, W) if grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe0916f-fecf-4ba6-b03f-a39f54b5a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These aren't needed until later but it's common/very-good practice to define globals at the top of any file/playbook\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "latent_dim = 10\n",
    "epochs = 30\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35961250-6515-4b67-aa7a-9db8f9b05706",
   "metadata": {},
   "source": [
    "# Part 1 Sobel Filters\n",
    "\n",
    "This short section will walk through using Sobel filters to perform \"edge detection\" on an input image.\n",
    "\n",
    "For this you will need to complete the Sobel filters themselves adnd then apply them to the input image that has been provided and analyze the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9879b6-5b46-48a4-8251-e971763d08dd",
   "metadata": {},
   "source": [
    "## 1.1 Lets analyze our input image and convert to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb42575-e1a0-481a-a243-eaa3dfc3d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First plot our input image for comparison.\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.title('Original Image')\n",
    "plt.imshow(image.squeeze())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b60942a-bc1e-43f3-9f94-ae8d4881d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to grayscale if it's an RGB image\n",
    "if image.ndim == 3:\n",
    "    # Simple average method for grayscale conversion\n",
    "    image = image.mean(axis=2)  # Shape: (H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438c977-5912-4983-8672-191387919818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a PyTorch tensor and normalize to [0, 1] if needed\n",
    "# Some images may already be normalized (if float type), so we check\n",
    "if image.max() > 1:\n",
    "    image = image / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eda4c8-d392-49ce-bdbb-fa935d0056d9",
   "metadata": {},
   "source": [
    "## 1.2 Construct the Sobel filters\n",
    "\n",
    "```\n",
    "The kernels Gx and Gy as covered in the last lecture:\n",
    "      _               _                   _                _\n",
    "     |                 |                 |                  |\n",
    "     | 1.0   0.0  -1.0 |                 |  1.0   2.0   1.0 |\n",
    "Gx = | 2.0   0.0  -2.0 |    and     Gy = |  0.0   0.0   0.0 |\n",
    "     | 1.0   0.0  -1.0 |                 | -1.0  -2.0  -1.0 |\n",
    "     |_               _|                 |_                _|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d30c9-0480-40e8-81e7-012a2f139fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add batch and channel dimensions (1, 1, H, W)\n",
    "image_tensor = torch.tensor(image, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "Gx = [ [,,], [], [] ] ## FINISH_ME ##\n",
    "Gy = [ [,,], [], [] ] ## FINISH_ME ##\n",
    "\n",
    "# Define Sobel kernels\n",
    "sobel_kernel_x = torch.tensor(Gx, dtype=torch.float32).view(1, 1, 3, 3)\n",
    "\n",
    "sobel_kernel_y = torch.tensor(Gy, dtype=torch.float32).view(1, 1, 3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43fec9c-8d12-4f71-bf31-f90fdadfe186",
   "metadata": {},
   "source": [
    "## 1.3 Apply Sobel and plot the result\n",
    "\n",
    "NB:\n",
    "When plotting the result of applying a filter it's always best to plot the RMS of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8185d3-e175-4cc0-a9cd-70de8d02e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Sobel filters using convolution\n",
    "edges_x = F.conv2d(image_tensor, sobel_kernel_x, padding=1)\n",
    "edges_y = F.conv2d(image_tensor, sobel_kernel_y, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facd8404-5edb-4ade-af08-535b15a27743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine edges to get the gradient magnitude\n",
    "edges = torch.sqrt(edges_x**2 + edges_y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6f28c6-9e2b-4bdc-b921-13ac9e57540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original and edge-detected images\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(image_tensor.squeeze().numpy(), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Sobel X (Horizontal Edges)')\n",
    "plt.imshow( ## FINISH_ME ##\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Sobel Y (Vertical Edges)')\n",
    "plt.imshow( ## FINISH_ME ##\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a532e500-9b7a-46ec-b47e-dab386d80c09",
   "metadata": {},
   "source": [
    "# Part 2 Understanding the PyTorch 2D CNN\n",
    "\n",
    "First we'll load the mnist numerical dataset, construct some 2D CNN and see if we understand the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac59014-0645-4289-85d4-58dec11bed6e",
   "metadata": {},
   "source": [
    "##  Part 2.1 Load the mnist dataset, this-time using transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd72cb-9992-4c89-ab28-e1d9b9299910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24295a6e-1bf8-4f4e-94eb-0512e4e9c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full training MNIST dataset\n",
    "dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "# Load Test Set\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Split into 90% train, 10% validation\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a7efb-9ba3-4e5b-a7ec-f6b02e93371d",
   "metadata": {},
   "source": [
    "## Part 2.2 Create some Conv 2D CNN and apply them to some example inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40391323-3238-460e-aec5-0bfc96382e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = random.randint(0, len(train_dataset) - 1)  # Select a random image\n",
    "image, label = train_dataset[random_idx]  # Random MNIST image\n",
    "\n",
    "# Add batch and channel dimensions for Conv2d (1, 1, 28, 28)\n",
    "input_image = image.unsqueeze(0)  # Shape: (1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd68e1-8b7e-4dba-933c-436dd91b9bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Apply Convolution with Different Configs\n",
    "def apply_conv(input_image, kernel_size, stride, padding):\n",
    "    # We want to use in_channels=1 and out_channels=1 and various inputs as defined above\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "    conv = nn.Conv2d(in_channels=1, out_channels=1, ## FINISH_ME ##\n",
    "    output = conv(input_image)\n",
    "    return output\n",
    "\n",
    "# Different Configurations\n",
    "configs = [\n",
    "    {\"kernel_size\": 3, \"stride\": 1, \"padding\": 0},\n",
    "    {\"kernel_size\": 3, \"stride\": 1, \"padding\": 2},\n",
    "    {\"kernel_size\": 5, \"stride\": 1, \"padding\": 0},\n",
    "    {\"kernel_size\": 5, \"stride\": 2, \"padding\": 2},\n",
    "    {\"kernel_size\": 3, \"stride\": 2, \"padding\": 1}\n",
    "]\n",
    "\n",
    "# Plot Input Image\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(f\"Original Image\\n(28x28)\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Apply Configurations and Plot Outputs\n",
    "for i, cfg in enumerate(configs):\n",
    "\n",
    "    output = apply_conv(input_image, cfg['kernel_size'], cfg['stride'], cfg['padding'])\n",
    "    output_shape = output.shape  # Shape: (1, 1, H_out, W_out)\n",
    "\n",
    "    plt.subplot(2, 3, i+2)\n",
    "    plt.imshow( ## FINISH_ME ##\n",
    "    plt.title(f\"Kernel: {cfg['kernel_size']}, Stride: {cfg['stride']}, Pad: {cfg['padding']}\\nShape: {output_shape[2]}x{output_shape[3]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27993e18-f971-4731-a1ab-b9c14b44cf03",
   "metadata": {},
   "source": [
    "# Part 3 Construct our data loaders and Variational Auto-Encoder model\n",
    "\n",
    "Our VAE model is designed to encode information from our "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed5de92-9fc8-4b43-a797-6ff6d6189088",
   "metadata": {},
   "source": [
    "## Part 3.1 Construct our data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043b734-c2ca-4ee7-be0c-dd7d2220c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for train and validate\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888fcc82-16fd-4821-9132-645b93b6673e",
   "metadata": {},
   "source": [
    "## Part 3.2 Construct our Variational Auto-Encoder class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a111bab-6d41-4e69-b00e-bcbea97c2ec9",
   "metadata": {},
   "source": [
    "Our Auto-Encoder class has several key features which we want to demonstrate.\n",
    "\n",
    "1. Decreasing dimension when Encoding information into latent-space.\n",
    "2. Increasing dimension when Decoding from latent-space.\n",
    "3. Symmetry between our encoder and decoder to help model training.\n",
    "4. Fixed Latent-Space dimension\n",
    "5. Weights initialized using 'sensible' defaults\n",
    "6. Use of the 'SiLU' https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html activation function\n",
    "\n",
    "Our encoder needs to output the value of the input image encoded into this latent-space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252342c9-e5a9-4718-8350-e9dddfa715ae",
   "metadata": {},
   "source": [
    "### 3.2.0 The re-parameterization trick is needed to make the Auto-Encoder work but is only used when assembling the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d866b9-8405-40c0-84b6-fc2bc2d3df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_base_model_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23b1e4a-978d-4e95-b5e9-b290959192ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reparameterization Trick\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cce69-41af-4c94-80aa-e662e89dfb5a",
   "metadata": {},
   "source": [
    "### 3.2.1 Build our Encoder\n",
    "\n",
    "We want to start from 1x28x28 and apply a filter which takes us to 16x28x28 then decreasing to 32x14x14 then 64x7x7 then finally we take this output and encode the final value onto our latent-space parameters via mu/logvar.\n",
    "\n",
    "It's common to use kernels of size 3 when sampling but not up/down-scaling and more common to use larger filters such as 4 when up-down-sampling to capture more information.\n",
    "\n",
    "With all that in mind you should get something similar to:\n",
    "\n",
    "1. input=1, output=16, kernel=3, stride=1, padding=1\n",
    "2. input=16, output=32, kernel=4, stride=2, padding=1\n",
    "3. input=32, output=64, kernel=4, stride=2, passing=1\n",
    "4. 64x7x7 -> 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3a2b7-e63a-4ea6-9bfa-e1c04e20850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Encoder with Swish (SiLU) and Batch Normalization\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # conv0 and bn0 are used by layer1\n",
    "        self.conv0 = nn.Conv2d( ## FINISH_ME )\n",
    "        self.bn0 = nn.BatchNorm2d(vae_base_model_dim)\n",
    "\n",
    "        # conv1 and bn1 are used by layer2\n",
    "        self.conv1 = nn.Conv2d( ## FINISH_ME )\n",
    "        self.bn1 = nn.BatchNorm2d(vae_base_model_dim*2)\n",
    "\n",
    "        # conv2 and bn2 are used by layer 3\n",
    "        self.conv2 = nn.Conv2d( ## FINISH_ME ##\n",
    "        self.bn2 = nn.BatchNorm2d(vae_base_model_dim*4)\n",
    "\n",
    "        # fc1 'projects' from filtered data to Latent-Space\n",
    "        self.fc1 = nn.Linear(vae_base_model_dim*4 * 7 * 7, 128)\n",
    "\n",
    "        # fc_my and fc_logvar are needed to use the re-param trick later\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "\n",
    "        # Lets make sure the model is initialized better than random\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1 we're not down-sampling we're performing feature extraction\n",
    "        x = F.silu(self.conv0(x))\n",
    "        # Layer 2 we want to down-sample reducing data to next layer by 50%\n",
    "        x = F.silu(self.bn1(self.conv1(x)))\n",
    "        # Layer 3 we're down-sampling again\n",
    "        x = ## FINISH_ME ##\n",
    "\n",
    "        # This is the equivalent of re-sizing the data flowing through the model\n",
    "        # This preserves the dim x[0] which is needed to preserve the batch-structure\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # This now projects the flattened data to 128-dim\n",
    "        x = F.silu(self.fc1(x))\n",
    "\n",
    "        # This finally projects down from 128 -> latent-dim\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # This iterates through all PyTorch objects created within this class\n",
    "        ## The mechanism for this is advanced but is made available by Python 'magic'\n",
    "        for m in self.modules():\n",
    "            # If we find a Conv2D class within our module, make sure we initialize this better\n",
    "            # https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            # If we find a Linear class lets initialize the parameters using xavier\n",
    "            # https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e9f9f5-dc6e-4984-9691-990e974447ca",
   "metadata": {},
   "source": [
    "### 3.2.2 Build our Decoder\n",
    "\n",
    "Starting from our latent-space we first need to project up to 64x7x7 then using filters, up-scale to 32x14x14, 16x28x28 then finally using an additional filter downscale back to 1x28x28.\n",
    "\n",
    "We want to make our decoder symmetrical to our encoder but in reverse. This helps with model stability during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1810625-2f8b-4370-82c1-ccc60f6cc4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN Decoder with LeakyReLU and Batch Normalization\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # fc1 & bn1 projects up from the Latent-Space to 128-dim\n",
    "        self.fc1 = nn.Linear(latent_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # fc2 & bn2 projects from 128-dim to model input\n",
    "        self.fc2 = nn.Linear(128, vae_base_model_dim*4 * 7 * 7)\n",
    "        self.bn2 = nn.BatchNorm1d(vae_base_model_dim*4 * 7 * 7)\n",
    "\n",
    "        # deconv0 & bn3 up-sample from Latent-Space dist\n",
    "        # We use ConvTranspose2d vs Conv2D \n",
    "        self.deconv0 = nn.ConvTranspose2d( ## FINISH_ME ## )\n",
    "        self.bn3 = nn.BatchNorm2d(vae_base_model_dim*2)\n",
    "\n",
    "        # deconv1 and bn4 up-sample from Latent-Space further\n",
    "        self.deconv1 = nn.ConvTranspose2d( ## FINISH_ME ## )\n",
    "        self.bn4 = nn.BatchNorm2d(vae_base_model_dim)\n",
    "\n",
    "        # deonv2 peforms the opposite of feature extraction, identifying key features from the up-scaling\n",
    "        self.deconv2 = nn.ConvTranspose2d(vae_base_model_dim, 1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Lets make sure the model is initialized better than random\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Lets project from Latent-Space to 128-dim\n",
    "        x = F.silu(self.bn1(self.fc1(z)))\n",
    "        # Lets project from 128-dim to up-scale filters\n",
    "        x = F.silu(self.bn2(self.fc2(x)))\n",
    "\n",
    "        # This is the equivalent to the opposite of the 'flatten' reize in the encoder\n",
    "        x = x.view(x.size(0), vae_base_model_dim*4, 7, 7)\n",
    "\n",
    "        # This up-scales from the Latent-Space projection to a larger image\n",
    "        x = F.silu(self.bn3(self.deconv0(x)))\n",
    "        # This up-scales again to give us output image sized data-streams\n",
    "        x = ## FINISH_ME ##\n",
    "\n",
    "        # This is the final 'projection' layer which extracts key features and makes an output image \n",
    "        x = torch.sigmoid(self.deconv2(x))\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # This iterates through all PyTorch objects created within this class\n",
    "        ## The mechanism for this is advanced but is made available by Python 'magic'\n",
    "        for m in self.modules():\n",
    "            # If we find a Conv2D class within our module, make sure we initialize this better\n",
    "            # https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            # If we find a Linear class lets initialize the parameters using xavier\n",
    "            # https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c26da8-58bd-4902-8c51-25d4ec9356f3",
   "metadata": {},
   "source": [
    "### 3.2.3 Build the final VAE model\n",
    "\n",
    "The VAE model itself is quite short. We just want to programatically 'connect' the Encoder and Decoder graphs through their latent-space and return the outputs needed by our loss function(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6421ff-a1ae-4016-93d1-51c44dccd8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational Autoencoder (VAE)\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Our VAE needs an Encoder and Decoder\n",
    "        # Both need to be constructed with knowledge of the required latent-dim\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The encoder returns mu and sigma(logvar) of our distribtuion in LS\n",
    "        mu, logvar = ## FINISH_ME ##\n",
    "\n",
    "        # We now want to re-parameterize to a single vector in LS\n",
    "        z = reparameterize(mu, logvar)\n",
    "\n",
    "        # The decoder is able to now take the single vector to re-construct an image\n",
    "        x_recon = ## FINISH_ME ##\n",
    "        # To train the model we need to return the final value\n",
    "        # AND intermediate values from the construction of our LS\n",
    "        return x_recon, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8634fa7a-d6d4-43c6-83a0-dbc5fab5c38d",
   "metadata": {},
   "source": [
    "## 3.3 Defining the Model Loss\n",
    "\n",
    "The total training loss from this model is the linear combination of the KL-divergence and the Reconstruction loss of the images themselves.\n",
    "\n",
    "The Reconstruction loss in this case is simply defined as the F.mse_loss https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae6049-a2e6-4741-85d7-d2088cd0626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function: Reconstruction Loss + KL Divergence\n",
    "def loss_function(x_recon, x, mu, logvar):\n",
    "    # The loss function relies on Model Output, Truth, mu and sigma\n",
    "\n",
    "    # First part of the loss is simply how 'bad' our output images are compared to 'truth'\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction='sum')\n",
    "\n",
    "    # We want to make sure our Latent-Space is encoded down to a 'Probability Space'\n",
    "    # This means we want to calculate the kl_loss of our LS-vector distributioncompared to a sampled normalized probaility\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return recon_loss + kl_loss, recon_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf0178-61a4-4cd8-87e7-f63def4e4076",
   "metadata": {},
   "source": [
    "# Part 4 Training our model\n",
    "\n",
    "As with our Classifier model we need to train our model to do something useful and extract information from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a885ab55-eeba-46fd-82cb-47fa4da3d028",
   "metadata": {},
   "source": [
    "## Part 4.1 training pre-requisits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7daac6-9dca-47d3-a196-57b6ffe3d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "device = torch.device(\"cuda\") # As with last week if you have access to anything 'non-cpu' I recommend using it here(!)\n",
    "\n",
    "# To train our model we need to first construct it, this means we can make a decision here on the latent-dim of the model\n",
    "vae = VAE(latent_dim).to(device)\n",
    "\n",
    "# To train the model we constructed we need to let the optimizer know about it\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f644f73-7504-4e17-8e38-32a97809ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to track the training loss and the loss from our validation dataset\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae5ff7-9bf3-4ec6-a0ea-3124f1bac360",
   "metadata": {},
   "source": [
    "## Part 4.2 Our training Loop\n",
    "\n",
    "It's good to prove that your model trains as expected. However the computational power required to train a proper model for this problem is quite expensive. (30min on a decent GPU or more!)\n",
    "\n",
    "With that in mind, feel free to only train over 2-3 epochs to demonstrate that your model is indeed reducing in loss per-epoch and compare the raw train and validation losses in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fe2891-386c-4f6e-94ee-7a1c23b8c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop over all epochs\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Make sure the model is in training mode\n",
    "    vae.train()\n",
    "\n",
    "    # (re-) set the training loss to be 0 for each epoch\n",
    "    train_loss = 0\n",
    "\n",
    "    # Loop over all batches in the train_loader\n",
    "    i=0\n",
    "    for x, _ in train_loader:\n",
    "\n",
    "        # Here we don't _need_ to know about the data labels\n",
    "        # In Python, it's common/good-practice to allocate\n",
    "        # returned parameters we don't care about to '_'\n",
    "\n",
    "        # Our training loop is no different to when training\n",
    "        # a classifier\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, mu, logvar = vae(x)\n",
    "\n",
    "        # Here we are calculating our training loss\n",
    "        # This depends on comparing:\n",
    "        # model output to input,   x_recon to x\n",
    "        # LS dist to Prob-Space,   mu&logvar to n-dim normal\n",
    "        loss, _ = loss_function( ## FINISH_ME ##\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        ## Useful for debugging, noisy for real training\n",
    "        ##print(f'Processing Batch: {i} of {len(train_loader)}')\n",
    "        ##i+=1\n",
    "\n",
    "    ## This section is new\n",
    "    ## We want to _evaluate_ our dataset once per epoch\n",
    "    ## This allows us to see if a model has over-trained or not\n",
    "\n",
    "    # Validation Loss\n",
    "    vae.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # As above, iterate over all data in the dataset\n",
    "        # Here we're using the Validation dataset\n",
    "        for x, _ in val_loader:\n",
    "            x = x.to(device)\n",
    "\n",
    "            x_recon, mu, logvar = vae(x)\n",
    "            loss, _ = loss_function( ## FINISH_ME ##\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # We DO NOT TRAIN over the VAE dataset so return from here\n",
    "\n",
    "    # We can now calculate the loss per-batch for both data (sub-)sets\n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_val_loss = ## FINISH_ME ##\n",
    "    # Store the values to examine them afterwards\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be13c9-c503-4ad1-9c68-5a77250faa53",
   "metadata": {},
   "source": [
    "## Part 4.3 Plot the losses from our training\n",
    "\n",
    "This allows us to make a statement about whether our model has over-trained or under-trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd22ab9-3e62-4142-9525-bb825ef657dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training vs Validation Loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(train_losses)+1), train_losses, label=\"Train Loss\", marker='o')\n",
    "plt.plot(range(1, len(val_losses)+1), val_losses, label=\"Validation Loss\", marker='s')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "#plt.xscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091db00d-6c2b-4da7-b75b-c7e5747c09f2",
   "metadata": {},
   "source": [
    "## Part 4.4 Examine the model performance\n",
    "\n",
    "We want to visualize what the output from our model is based on inputs.\n",
    "\n",
    "If you prefer you can run this after loading a pre-saved model to see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb3e11-491e-4d0a-b997-0e96faba83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Input vs Reconstructed Images\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    sample_data, _ = next(iter(train_loader))\n",
    "    sample_data = sample_data[:8].to(device)  # Select 8 images\n",
    "    reconstructed, _, _ = vae(sample_data)\n",
    "\n",
    "# Convert to CPU for visualization\n",
    "sample_data = sample_data.cpu()\n",
    "reconstructed = reconstructed.cpu()\n",
    "\n",
    "# Plot Input vs Output\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 4))\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(sample_data[i].squeeze(), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].imshow(reconstructed[i].squeeze(), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_title(\"Original Images\", fontsize=12)\n",
    "axes[1, 0].set_title(\"Reconstructed Images\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aafbfce-f9e7-453d-8d4b-7940346c6f10",
   "metadata": {},
   "source": [
    "# Part 5 Load and use a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6d7db8-2ad4-49df-988c-1e731273da39",
   "metadata": {},
   "source": [
    "## Part 5.1 Load/Save Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f87141-2b48-4670-8731-e385318da927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're happy with your model from training you can save it's progress or the final model with the following\n",
    "torch.save(vae, 'trained_vae_model.pth')\n",
    "\n",
    "## IF YOU WANT TO USE A PRE-TRAINED MODEL AFTER HERE THERE IS ONE PROVIDED\n",
    "\n",
    "# Load the entire model\n",
    "vae_entire_loaded = torch.load('trained_vae_model.pth')\n",
    "vae_entire_loaded.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50944e23-a02f-4ebd-acb0-a666daf84016",
   "metadata": {},
   "source": [
    "## Part 5.2 Lets use our pre-trained model to generate some images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712eaa46-ecf6-4e6c-addb-5dedf171a81b",
   "metadata": {},
   "source": [
    "### Part 5.2.1 Lets construct some random vectors in Prob-Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e70b1-0480-4492-9816-c0001f3e5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate New Images\n",
    "num_images = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ccff5-5440-425c-9e98-b9c056dc63cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random latent vectors from standard normal distribution\n",
    "rand_mu = torch.randn(num_images, ## FINISH_ME ##\n",
    "rand_log = torch.randn(num_images, ## FINISH_ME ##\n",
    "\n",
    "latent_vectors = reparameterize( ## FINISH_ME ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d31676-1216-4643-8aac-4600bb4667eb",
   "metadata": {},
   "source": [
    "### Part 5.2.2 Now we have some random vectors use the Decoder to build some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ecee7-37ac-4e54-98ef-84999160fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the decoder to generate images\n",
    "with torch.no_grad():\n",
    "    latent_vectors = latent_vectors.to(device)\n",
    "    generated_images = vae_entire_loaded.decoder( ## FINISH_ME ##\n",
    "    generated_images = generated_images.view(-1, 28, 28)  # Reshape to (28x28)\n",
    "    generated_images = generated_images.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967a1a3-f3c7-453a-89d4-7adef6988c63",
   "metadata": {},
   "source": [
    "### Part 5.2.3 Now lets plot the output of these images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651cf6d3-601f-42f4-bad3-cabd29cd2440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Generated Images\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(1, num_images, i+1)\n",
    "    plt.imshow(generated_images[i], cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle(\"Generated Images from Pre-trained VAE\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5585f-97cd-4355-8812-b6c037d8df88",
   "metadata": {},
   "source": [
    "# Part 6 Examining the trained model Latent-Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faff319-b588-40a5-904a-2fc22bcb125d",
   "metadata": {},
   "source": [
    "## Part 6.1 Make sure our model is in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefc0678-5d62-4d13-915b-289403b078cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model is in evaluation mode\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3569483-e92d-40b1-a4af-ec09c719c46f",
   "metadata": {},
   "source": [
    "## Part 6.2 Iterate over the test dataset and collect the distribution of latent vectors & losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3920a98-c0c4-4f55-8c1b-0ff87cd57d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect latent representations and labels\n",
    "latent_vectors = []\n",
    "labels = []\n",
    "all_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6599d8-41c3-49bd-ab9f-0e9053c222c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        # This time we want to track which number ended up where in the latent-space\n",
    "        x = x.to(device)\n",
    "\n",
    "        # Evaluate the encoder to get the latent-space vector \n",
    "        mu, logvar = ## FINISH_ME ##\n",
    "        z = reparameterize(mu, logvar)  # Use reparameterization trick to get final z\n",
    "\n",
    "        for image in x:\n",
    "            recon_img = vae.decoder(z)\n",
    "            img_loss, _ = loss_function( ## FINSH_ME ##\n",
    "            all_losses.append(img_loss.item())\n",
    "\n",
    "        # Store the vectors and labels for plotting\n",
    "        latent_vectors.append(z.cpu().numpy())\n",
    "        labels.append(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbbbd73-5041-4920-aa2f-31ff948c7e2a",
   "metadata": {},
   "source": [
    "## Part 6.3 Plot the distribution of per-image losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc160d9-17f7-4975-a8f8-5dd21bb33b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot the distribution of losses for the whole test dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_losses, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.title(\"Distribution of VAE Losses on Test Dataset\", fontsize=16)\n",
    "plt.xlabel(\"Loss Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7a07b-c87b-4c06-9cbe-12220519927a",
   "metadata": {},
   "source": [
    "### What would you expect if we passed a b&w image of a face through our AE?\n",
    "\n",
    "`## FINISH_ME ##`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb6eee6-3974-4f79-9d63-9ec00d751dd3",
   "metadata": {},
   "source": [
    "## Part 6.4 Explore the Latent-Space distribution for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5e4d1-abfc-4dbd-a9cb-75cac1356b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to arrays\n",
    "latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
    "labels = np.concatenate(labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625b4160-6eeb-488c-bf02-e4c56abb18d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code just gets the different combination of projections from n-dim down to 3D\n",
    "\n",
    "# Get all 3D combinations from our latent space\n",
    "latent_dim = latent_vectors.shape[1]\n",
    "three_d_combinations = list(itertools.combinations(range(latent_dim), 3))\n",
    "\n",
    "# Create figure grid to plot all projections\n",
    "## Need to know how many projections we need, for 10-dim LS this equates to 120 possible images(!)\n",
    "num_projections = len(three_d_combinations)\n",
    "cols = 4  # Number of columns in the grid\n",
    "rows = (num_projections // cols) + (num_projections % cols > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ab66d2-d618-4617-9ad6-e80fbcc0918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make a large meta-image to start with\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "\n",
    "# Iterate through all possible 3D projections\n",
    "for i, (dim1, dim2, dim3) in enumerate(three_d_combinations):\n",
    "\n",
    "    # Lets start a new sub-plot\n",
    "    ax = fig.add_subplot(rows, cols, i+1, projection='3d')\n",
    "\n",
    "    # For each number class lets run through our latent-vector values\n",
    "    for digit in range(10):\n",
    "        # This is a way of selecting only the images corresponding to this class\n",
    "        mask = labels == digit\n",
    "        # Lets draw a new class of images scattered within this 3D latent space\n",
    "        ax.scatter(latent_vectors[mask, dim1], latent_vectors[mask, dim2], latent_vectors[mask, dim3],\n",
    "                   label=f\"{digit}\", alpha=1.0, edgecolors='none', s=5)\n",
    "\n",
    "    # Lets add some info about this sub-plot\n",
    "    ax.set_xlabel(f\"Dim {dim1+1}\")\n",
    "    ax.set_ylabel(f\"Dim {dim2+1}\")\n",
    "    ax.set_zlabel(f\"Dim {dim3+1}\")\n",
    "    ax.set_title(f\"Projection ({dim1+1}, {dim2+1}, {dim3+1})\")\n",
    "    ax.legend(loc=\"upper right\", fontsize=12)\n",
    "\n",
    "# Plot the final image\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564e55ce-5f68-4aef-a4ba-7a3032992022",
   "metadata": {},
   "source": [
    "# Part 7 Train a VAE over CIFAR10\n",
    "\n",
    "I've included the bare-minimum to load the CIFAR10 dataset using the PyTorch built-ins.\n",
    "\n",
    "This dataset is composed of 3x32x32 images which we can use to train a VAE.\n",
    "\n",
    "This means you will likely need more CPU time than training on the mnist numerical data but feel free to try other model dimensions and such to see how well you can build a VAE for CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb903c3-1e90-48df-ab99-907951fae2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Transform (Convert to Tensor & Normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts images to PyTorch tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize RGB channels\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac565d-accf-411f-a330-1e64882d8900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training Dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Load Test Dataset\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab3fbb-7d9f-4e27-9c75-33dc9dd9542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for batching & shuffling\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744b95f-1435-4784-8f36-b0ef8519a98e",
   "metadata": {},
   "outputs": [],
   "source": [
    " ## FINISH_ME ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
